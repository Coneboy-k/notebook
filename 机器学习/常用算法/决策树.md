### 流程
决策树 (decision tree) 是一类常见的机器学习方法.以二分类任务为例，我 们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本 分类的任务，可看作对"当前样本属于正类吗?"这个问题的"决策"或"判 定"过程.顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决 策问题时一种很自然的处理机制.例如，我们要对"这是好瓜吗?"这样的问题 进行决策时，通常会进行一系列的判断或"子决策"我们先看"它是什么颜 色?"，如果是"青绿色"，则我们再看"它的根蒂是什么形态?"，如果是"蜷 缩"，我们再判断"它敲起来是什么声音?"，最后?我们得出最终决策:这是个好瓜
### 简介
决策树主要用于分类问题,比如,判断西瓜是否是好瓜,我们需要结合西瓜的多个属性,进行综合判断,一般我们会首先敲两下,听听声音,声音混沌,那么这个瓜很有可能是好瓜,但是这个不能让我们百分百确定,我们还得考虑别的属性,比如它的颜色,根蒂等等,通过综合考虑使我们更加确信是好瓜,也就是综合考虑这个是好瓜的概率.比如通过一个属性(声音)确定它是好瓜的概率50%,然后再考虑它的另一个属性(颜色),使概率增加到60% ,然后一步一步使概率逐步增加.
### 方法
由上可知,我们通过决策树判断是否是好瓜,等方法是不断判断属性,增加分类成功的概率,然而一个西瓜有多个属性,比如颜色,敲击声音,形状等等,如何判断哪些属性有利于区分好瓜,也就是增加分类的成功率呢?这里就用到了一些方法:
### 信息增益
比如我们首先根据样本中西瓜的形状判断好瓜还是坏的,然而我们可想而知,基本上是个西瓜就是圆的,通过形状分完类,发现样本中的数据全被分到一块了(假设这个分类结果集为B), 我们还是不知道哪些是好的坏的.所以说形状这个属性不利于我们区分西瓜好坏.然而我们可以看到结果B是否符合要求,那么机器就不容易知道是否符合要求了,所以我们要定义一下怎么判断结果是否满意.于是,我们就用纯度来表示.比如通过形状分完后,结果里面有5个好的,5个坏的,也就是比较乱什么分类的都有,那么他的每一个分类的纯度就不够(比如酒,酒精多了酒精的纯度就高,水的纯度变低),而通过西瓜的声音判断的话,声音浑浊中的结果中有4个好瓜,一个坏瓜,那么好瓜的纯度就高,说明根据声音判断它的效果就比较好,而这个纯度怎么计算,就是信息熵
#### 信息熵
##### 描述
描述样本中分类(标签)的类别数量,分类越多,熵越大
,也就是纯度越低
##### 公式
```math
Entropy(S)=-\sum_{i=1}^mp_ilog_2p_i
```
* S:整个集合
* m: 分类数量,有多少种分类
* pi:第i类所占数量在整个集合中的比例
#### 信息增益
用过信息熵,我们可以验证结果是否符合要求,但是多个属性,我们首先判断哪一个呢?于是我们也要选择一个分类效果最好的一个属性,先判断它,比如,我们一般挑西瓜先敲两下,声音低沉,那我们基本心里有底这个是熟的瓜的概率70%,然后我们再看看别的属性,使我们更确定些,但是如果我们一开始先看瓜的根蒂,看完了我们也不确定,然后再看别的属性,也不确定,说明这些属性对于判断结果的效果不大.也就是信息增益低.
##### 描述
描述一个特征对于分类的作用大小,如果通过一个特征能把所有样本分开,那么说明这个特征的作用比较大,也就是信息增益大.
##### 公式
```math
Gain(S,A) = Entropy(S) - \sum_{v \in values(A)}\frac{S_v}{S}Entropy(S_v)
```
* S: 整个集合
* A: 某个特征(比如颜色)
* v: 特征的取值(比如白,黑)
* Sv: 特征值的数量(比如总共100,白72,那么Sv =72)
* Entropy(Sv): 在Sv中的香农熵,算占比按照分类在Sv中的占比,不涉及S
##### 总结
经过计算,某个特征的增益越大,说明使用它来分类比较靠谱,因为它分类后的内容基本正确,反而信息增益越小,说明它分类出来的结果还不如不分类,不靠谱.
### 增益率和基尼指数
通过信息增益和信息熵,有利于我们判断某个属性对于分类的重要性(决定性),但是,容易出现一个问题: 比如我们的数据中经常有编号(ID)这个属性,如果我们按照id进行分类,我们会发现,它的增益很高,比如id为1是好的,2是坏的,3是好的,这样进行分类,我们会发现id这个属性,有3个分支(1,2,3),然后每个分支下的结果只有一个而且纯度是100%,然而我们知道,属性ID对于分类是没有任何帮助的,但是机器不知道,所以决策树C4.5算法和CART决策树分别使用增益率和基尼指数来进行计算.
#### 增益率
##### 概念
用来选择最优划分属性,比如ID属性,信息增益很高,但是分支多,那么我们可以通过`信息增益/分支数量`这样它的结果就比较低,降低了ID属性的重要性.增益率的计算类似这样.
##### 公式
```math
Gain_radio(S,A) = \frac{Gain(S,A)}{IV(A)}

IV(A) = -\sum_{v=1}^{V}\frac{|Sv|}{|S|}log_2{\frac{|Sv|}{|S|}}
```
* S: 整个集合
* A: 某个特征(比如颜色)
* v: 特征的取值(比如白,黑)
* Sv: 特征值的数量(比如总共100,白72,那么Sv =72)
* Gain(S,A)为信息增益
* IV(A)称为A的固有值,A属性的分支越多,那么IV的值越大

从上可以看出,增益率对于分支数量少的有所偏好.C4.5算法,并不直接采用增益率决定,而是先取信息增益的平均值,然后取大于平均值的属性中增益率最高的属性.

#### 基尼指数
基尼指数用途和增益率一样,只不过,算法不同.
##### 公式
```math
Gini(S) = \sum^{|y|}_{k=1}\sum^{}_{k'!=k}p_kp_{k'}    

Gini_index(S,A) = \sum^{v}_{v=1}\frac{|Sv|}{|S|}Gini(S^v)
```
* S: 整个集合
* A: 某个特征(比如颜色)
* v: 特征的取值(比如白,黑)
* Sv: 特征值的数量(比如总共100,白72,那么Sv =72)
* k,ki 待研究

直观来说， Gini(S) 反映了从数据集 S 中随机抽取两个样本，其类别标记 不一致的概率.因此， Gini(S) 越小，则数据集 S的纯度越高.

于是，我们在候选属性集合 A 中，选择那个使得划分后基尼指数最小的属
性作为最优划分属性


